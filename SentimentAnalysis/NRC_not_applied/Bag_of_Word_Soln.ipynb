{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this is to create bag of words and then matching each word to the dictionary once in order to minimize workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "plt.style.use('fivethirtyeight')\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add NRC data\n",
    "filepath = \"/Users/shimengfeng/Documents/Master_Columbia/Fall 2019/capstone/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\"\n",
    "emolex_df = pd.read_csv(filepath,  names=[\"word\", \"emotion\", \"association\"], skiprows=45, sep='\\t')\n",
    "# emolex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for words that do not have any emotion associated, delete them from dict and create a more cleaned one\n",
    "df_count = emolex_df.groupby('word').sum().reset_index()\n",
    "word_with_emotion_list = df_count[df_count.association !=0].word.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a sample data as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] =\\\n",
    "'/Users/shimengfeng/Documents/Master_Columbia/Fall 2019/capstone/\\\n",
    "dsi-capstone-f19-group-1-6c986cf239c5.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery valid_tweets\n",
    "SELECT DISTINCT id, full_text_cleaned\n",
    "FROM tweets.all_valid_tweets\n",
    "where lang = 'en'\n",
    "limit 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501537798110072800</td>\n",
       "      <td>send shockwaves w hands up in regards to the death of michael brown ynt dkp    http …</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>499265945873838100</td>\n",
       "      <td>stop being surprised that president hasnt spoken about michaelbrown but spoke on robinwilliams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>504061933772095500</td>\n",
       "      <td>melissa harris perry destroys time’s joe klein’s attempt to malign michael brown $URL$ tcot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>504415277790949400</td>\n",
       "      <td>has the cdc learned nothing from ferguson? cdcwhistleblower fraud is like africanamericans being targeted by police …</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>499762501454544900</td>\n",
       "      <td>blessings and safety to the people on the front line capturing situation in furguson your words are our strongest weapon! mikebrown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  \\\n",
       "0  501537798110072800   \n",
       "1  499265945873838100   \n",
       "2  504061933772095500   \n",
       "3  504415277790949400   \n",
       "4  499762501454544900   \n",
       "\n",
       "                                                                                                                      full_text_cleaned  \n",
       "0  send shockwaves w hands up in regards to the death of michael brown ynt dkp    http …                                                 \n",
       "1  stop being surprised that president hasnt spoken about michaelbrown but spoke on robinwilliams                                        \n",
       "2  melissa harris perry destroys time’s joe klein’s attempt to malign michael brown $URL$ tcot                                           \n",
       "3  has the cdc learned nothing from ferguson? cdcwhistleblower fraud is like africanamericans being targeted by police …                 \n",
       "4  blessings and safety to the people on the front line capturing situation in furguson your words are our strongest weapon! mikebrown   "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform bag of word on the text\n",
    "# valid_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean additional information from the tweet\n",
    "\n",
    "#clean number and URL\n",
    "valid_tweets['final_text'] = valid_tweets['full_text_cleaned'].str.replace(r\"[0-9]+\", \"\").\\\n",
    "apply(lambda x: x.replace('$URL$',''))\n",
    "\n",
    "#remove stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "valid_tweets['final_text'] = valid_tweets['final_text'].apply(lambda x: ' '.join([item for item in word_tokenize(x) if item not in stop_words]))\n",
    "  \n",
    "# valid_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply bag of words on the cleaned text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(valid_tweets['final_text'])\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(X.toarray())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,unique_word_list].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only recreate the columns have key words match the dictionary\n",
    "word_list = vectorizer.get_feature_names()\n",
    "unique_word_list = [word_list.index(word) for word in word_list if word in word_with_emotion_list]\n",
    "matrix_word_header = np.array(word_list)[unique_word_list] # get the header\n",
    "#get the words that are non-zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1791)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = X[:,unique_word_list].toarray()\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a dict that with each emotion as key\n",
    "# df_dict = emolex_df[emolex_df.association!=0][['word','emotion']]\n",
    "# emotion_dict= df_dict.groupby('word').apply(lambda x: x.drop('word', axis=1).to_dict('list')).to_dict()\n",
    "# emotion_dict            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dictionary - position-word-emotion\n",
    "# tweets_emotion_dict={}\n",
    "# keys = list(np.array(word_list)[unique_word_list])\n",
    "# for word in keys:\n",
    "#     tweets_emotion_dict.update({word:emotion_dict[word]['emotion']})\n",
    "#     # obtain all the emotions associate with the word\n",
    "# tweets_emotion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = emolex_df.emotion.unique()\n",
    "emotion_dict = {}\n",
    "for emotion in emotions:\n",
    "    emotion_df = emolex_df[emolex_df.emotion ==emotion]\n",
    "    emotion_df = emotion_df[emotion_df.association ==1]\n",
    "    emotion_dict[emotion] = emotion_df['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aberration', 'abort', 'abuse', 'account', 'accountable',\n",
       "       'accounts', 'action', 'actual', 'advice', 'aggressive', 'agree',\n",
       "       'aid', 'alive', 'ambulance', 'amen', 'amnesty', 'amour', 'anchor',\n",
       "       'angel', 'anger', 'angry', 'anonymous', 'armed', 'armor',\n",
       "       'armored', 'arrest', 'art', 'ashamed', 'ass', 'assassination',\n",
       "       'assault', 'atrocious', 'attack', 'attacking', 'attempt',\n",
       "       'attention', 'attorney', 'august', 'autopsy', 'avoid', 'awful',\n",
       "       'axiom', 'baby', 'bad', 'badly', 'bankrupt', 'beautiful', 'beauty',\n",
       "       'bee', 'believed', 'birth', 'bitch', 'black', 'blackness', 'blame',\n",
       "       'bless', 'blind', 'bloody', 'blurred', 'bomber', 'bounty', 'boy',\n",
       "       'boycott', 'break', 'breakdown', 'brilliant', 'broke', 'brother',\n",
       "       'brutality', 'brute', 'budget', 'bully', 'bury', 'busted', 'calls',\n",
       "       'calm', 'camouflage', 'candidate', 'cannon', 'captain', 'case',\n",
       "       'center', 'change', 'chaos', 'child', 'choir', 'chorus', 'church',\n",
       "       'civil', 'clean', 'clue', 'cold', 'comfort', 'coming', 'committee',\n",
       "       'communist', 'community', 'completely', 'comprehend', 'concerned',\n",
       "       'confirmed', 'confront', 'conspiracy', 'contact', 'continue',\n",
       "       'contrary', 'convenient', 'cool', 'cop', 'coroner', 'corruption',\n",
       "       'county', 'court', 'cover', 'crazy', 'credit', 'crew', 'crime',\n",
       "       'criminal', 'criminality', 'crisis', 'cry', 'crying', 'culture',\n",
       "       'customer', 'damage', 'damn', 'dangerous', 'deadly', 'dear',\n",
       "       'death', 'debate', 'deception', 'defend', 'defendant', 'defense',\n",
       "       'degree', 'delicious', 'demand', 'denied', 'depend', 'depressing',\n",
       "       'deserted', 'deserve', 'determined', 'devastating', 'die',\n",
       "       'differently', 'difficulties', 'disabled', 'discussion',\n",
       "       'disgusting', 'disobedience', 'displeasure', 'disregard',\n",
       "       'disrespect', 'distract', 'distress', 'disturbed', 'diversion',\n",
       "       'doctor', 'donation', 'doubt', 'duke', 'dying', 'eager', 'edition',\n",
       "       'effective', 'endanger', 'enforce', 'enforcement', 'entertainment',\n",
       "       'err', 'ethical', 'evade', 'excess', 'excuse', 'execution',\n",
       "       'executioner', 'exile', 'expected', 'expert', 'explain', 'explode',\n",
       "       'extend', 'eyewitness', 'fact', 'facts', 'failing', 'fair',\n",
       "       'faith', 'fall', 'fatal', 'father', 'favorite', 'feature',\n",
       "       'feeling', 'felon', 'fight', 'fighting', 'finally', 'fire', 'fled',\n",
       "       'flee', 'flying', 'focus', 'follower', 'food', 'force', 'forget',\n",
       "       'forward', 'found', 'freedom', 'friend', 'full', 'funeral', 'gang',\n",
       "       'gear', 'general', 'ghost', 'giving', 'glad', 'god', 'good',\n",
       "       'gospel', 'government', 'governor', 'grandfather', 'grandmother',\n",
       "       'grave', 'green', 'ground', 'guard', 'guess', 'guilt', 'guilty',\n",
       "       'gun', 'happen', 'hate', 'havoc', 'hearing', 'heavily', 'hell',\n",
       "       'hero', 'hide', 'highest', 'hit', 'holocaust', 'holy', 'honest',\n",
       "       'honor', 'hope', 'horribly', 'horrific', 'household', 'humanity',\n",
       "       'hunter', 'hurt', 'idiot', 'ignorant', 'ill', 'illegal',\n",
       "       'immediately', 'important', 'improvement', 'incident', 'including',\n",
       "       'inclusive', 'inflict', 'information', 'injured', 'injury',\n",
       "       'injustice', 'innocence', 'innocent', 'inquiry', 'insane',\n",
       "       'install', 'instructions', 'interested', 'interesting',\n",
       "       'interrupt', 'intimidation', 'invasion', 'investigate',\n",
       "       'investigation', 'isolated', 'jail', 'job', 'john', 'join', 'joke',\n",
       "       'journalism', 'journalist', 'jury', 'justice', 'juvenile', 'kill',\n",
       "       'killing', 'kind', 'kindness', 'king', 'knowing', 'lack', 'land',\n",
       "       'landmark', 'larger', 'late', 'laughing', 'law', 'lawsuit',\n",
       "       'lawyer', 'leader', 'learn', 'leave', 'legal', 'letter', 'library',\n",
       "       'lightning', 'lines', 'long', 'loot', 'lose', 'losing', 'lost',\n",
       "       'love', 'mace', 'machine', 'mad', 'madness', 'main', 'major',\n",
       "       'march', 'martial', 'mayor', 'medical', 'mess', 'military',\n",
       "       'militia', 'misconduct', 'missing', 'modest', 'money', 'moron',\n",
       "       'mother', 'mourning', 'murder', 'music', 'nation', 'necessity',\n",
       "       'neglect', 'neighbor', 'occupy', 'officer', 'official',\n",
       "       'opportunity', 'oppression', 'outrage', 'outrageous', 'overflow',\n",
       "       'pain', 'pastor', 'patrol', 'pay', 'peace', 'peaceful', 'peril',\n",
       "       'plan', 'planning', 'police', 'policeman', 'pow', 'powerful',\n",
       "       'praise', 'pray', 'preliminary', 'prepared', 'preparedness',\n",
       "       'prescient', 'present', 'president', 'pretty', 'problem',\n",
       "       'productive', 'progress', 'projectiles', 'promise', 'proof',\n",
       "       'proper', 'prosecution', 'protect', 'protected', 'protecting',\n",
       "       'proud', 'prove', 'proven', 'public', 'pull', 'question', 'quiet',\n",
       "       'quote', 'rage', 'rape', 'reading', 'ready', 'real', 'reason',\n",
       "       'recruits', 'refuse', 'refused', 'related', 'relevant', 'reliable',\n",
       "       'relief', 'religion', 'remorse', 'reporter', 'resign',\n",
       "       'resistance', 'resources', 'respect', 'respects', 'responsible',\n",
       "       'rest', 'result', 'revenge', 'revolt', 'revolutionary', 'reward',\n",
       "       'rifle', 'riot', 'robber', 'robbery', 'rooted', 'rule', 'ruth',\n",
       "       'safe', 'salute', 'sarcasm', 'school', 'seek', 'selfish', 'sense',\n",
       "       'senseless', 'serve', 'shameful', 'share', 'shattered', 'shelter',\n",
       "       'shining', 'shit', 'shoot', 'shooter', 'shooting', 'shot', 'show',\n",
       "       'sick', 'slam', 'small', 'smile', 'solidarity', 'solution',\n",
       "       'special', 'speech', 'spike', 'spirit', 'spoke', 'standing',\n",
       "       'standoff', 'star', 'start', 'statement', 'steady', 'steal',\n",
       "       'stealing', 'stinging', 'store', 'strategic', 'struggle', 'stupid',\n",
       "       'stupidity', 'subject', 'suck', 'supply', 'supporting', 'supreme',\n",
       "       'surprised', 'surrender', 'surveillance', 'suspect', 'system',\n",
       "       'talk', 'taught', 'tax', 'teach', 'team', 'tension', 'terrible',\n",
       "       'terribly', 'terrorism', 'testimony', 'thankful', 'theft',\n",
       "       'thought', 'threat', 'threaten', 'threatening', 'thug', 'time',\n",
       "       'tired', 'tomorrow', 'top', 'touched', 'tough', 'track', 'tragic',\n",
       "       'traveling', 'tribute', 'trip', 'true', 'trust', 'truth',\n",
       "       'truthful', 'turmoil', 'tyrannical', 'unable', 'unacceptable',\n",
       "       'unbelievable', 'unique', 'united', 'university', 'unjust',\n",
       "       'unprecedented', 'unrest', 'unsafe', 'untold', 'uprising', 'usual',\n",
       "       'vacation', 'validity', 'verified', 'versus', 'veteran', 'vice',\n",
       "       'victor', 'vigil', 'vindicate', 'violence', 'violent', 'visit',\n",
       "       'vital', 'volunteers', 'vote', 'wait', 'wanting', 'war', 'warfare',\n",
       "       'warning', 'watch', 'watchdog', 'wear', 'weight', 'weird', 'white',\n",
       "       'witness', 'wont', 'word', 'words', 'working', 'worse', 'worth',\n",
       "       'wrong', 'young', 'youth'], dtype='<U22')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_word_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select one emotion and get the matrix relates to the emotion\n",
    "def search_emotion_matrix (emotion_dict, emotion, matrix_word_header, tweets):\n",
    "    emotion_word = np.array(emotion_dict[emotion]) # get all words relates to one emotion\n",
    "#     print(emotion_word)\n",
    "    # get the position of the matrix\n",
    "    word_position = np.where(np.isin(matrix_word_header, emotion_word)==True)\n",
    "    tweets_emotion_words = matrix_word_header[word_position]\n",
    "    #get the updated matrix\n",
    "    tweets_emotion = tweets[:,word_position[0]]\n",
    "    # get non_zero output\n",
    "    non_zero_count = np.nonzero(tweets_emotion)\n",
    "    return non_zero_count, tweets_emotion_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the ids and convert\n",
    "ids = valid_tweets.id.tolist()\n",
    "ids = [str(i) for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f9d1520e9c22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create the final output dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0memotion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnon_zero_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets_emotion_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_emotion_matrix\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0memotion_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix_word_header\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mid_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_zero_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get all id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_emotion_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_zero_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "# create the final output dictionary \n",
    "for emotion in emotions: \n",
    "    non_zero_count, tweets_emotion_words = search_emotion_matrix (emotion_dict, emotion, matrix_word_header, tweets)\n",
    "    id_keys = np.array(ids)[non_zero_count[0]] # get all id\n",
    "    words_value = tweets_emotion_words[non_zero_count[1]]\n",
    "    new_dict = {}\n",
    "    for (key, value) in zip(id_keys,words_value):\n",
    "        if key in new_dict:\n",
    "            new_dict[key].append(value)\n",
    "        else:\n",
    "            new_dict[key] = [value]\n",
    "    #save the created dictionary file as pickle\n",
    "    with open('BOG_results/NRC_output_{}.pickle'.format(emotion),'wb') as f:\n",
    "        pickle.dump(new_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
